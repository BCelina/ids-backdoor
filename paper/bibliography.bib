
@article{radford_network_2018,
	title = {Network {Traffic} {Anomaly} {Detection} {Using} {Recurrent} {Neural} {Networks}},
	abstract = {We show that a recurrent neural network is able to learn a model to represent sequences of communications between computers on a network and can be used to identify outlier network traffic. Defending computer networks is a challenging problem and is typically addressed by manually identifying known malicious actor behavior and then specifying rules to recognize such behavior in network communications. However, these rule-based approaches often generalize poorly and identify only those patterns that are already known to researchers. An alternative approach that does not rely on known malicious behavior patterns can potentially also detect previously unseen patterns. We tokenize and compress netflow into sequences of "words" that form "sentences" representative of a conversation between computers. These sentences are then used to generate a model that learns the semantic and syntactic grammar of the newly generated language. We use Long-Short-Term Memory (LSTM) cell Recurrent Neural Networks (RNN) to capture the complex relationships and nuances of this language. The language model is then used predict the communications between two IPs and the prediction error is used as a measurement of how typical or atyptical the observed communication are. By learning a model that is specific to each network, yet generalized to typical computer-to-computer traffic within and outside the network, a language model is able to identify sequences of network activity that are outliers with respect to the model. We demonstrate positive unsupervised attack identification performance (AUC 0.84) on the ISCX IDS dataset which contains seven days of network activity with normal traffic and four distinct attack patterns.},
	urldate = {2019-07-30},
	journal = {arXiv:1803.10769 [cs]},
	author = {Radford, Benjamin J. and Apolonio, Leonardo M. and Trias, Antonio J. and Simpson, Jim A.},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.10769},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	annote = {Comment: Prepared for the 2017 National Symposium on Sensor and Data Fusion},
	file = {arXiv\:1803.10769 PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/XMB4GLA5/Radford et al. - 2018 - Network Traffic Anomaly Detection Using Recurrent .pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/QJ9R9SV7/1803.html:text/html}
}

@article{meghdouri_analysis_2018,
	title = {Analysis of {Lightweight} {Feature} {Vectors} for {Attack} {Detection} in {Network} {Traffic}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	abstract = {The consolidation of encryption and big data in network communications have made deep packet inspection no longer feasible in large networks. Early attack detection requires feature vectors which are easy to extract, process, and analyze, allowing their generation also from encrypted traffic. So far, experts have selected features based on their intuition, previous research, or acritically assuming standards, but there is no general agreement about the features to use for attack detection in a broad scope. We compared five lightweight feature sets that have been proposed in the scientific literature for the last few years, and evaluated them with supervised machine learning. For our experiments, we use the UNSW-NB15 dataset, recently published as a new benchmark for network security. Results showed three remarkable findings: (1) Analysis based on source behavior instead of classic flow profiles is more effective for attack detection; (2) meta-studies on past research can be used to establish satisfactory benchmarks; and (3) features based on packet length are clearly determinant for capturing malicious activity. Our research showed that vectors currently used for attack detection are oversized, their accuracy and speed can be improved, and are to be adapted for dealing with encrypted traffic.},
	language = {en},
	number = {11},
	urldate = {2019-07-30},
	journal = {Applied Sciences},
	author = {Meghdouri, Fares and Zseby, Tanja and Iglesias, Félix},
	month = nov,
	year = {2018},
	keywords = {feature selection, network attack detection, supervised learning},
	pages = {2196},
	file = {Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/DLBHCM7Q/Meghdouri et al. - 2018 - Analysis of Lightweight Feature Vectors for Attack.pdf:application/pdf;Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/EFNANYEY/2196.html:text/html}
}

@article{liu_fine-pruning:_2018,
	title = {Fine-{Pruning}: {Defending} {Against} {Backdooring} {Attacks} on {Deep} {Neural} {Networks}},
	shorttitle = {Fine-{Pruning}},
	abstract = {Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0\% with only a 0.4\% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.},
	urldate = {2019-08-20},
	journal = {arXiv:1805.12185 [cs]},
	author = {Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
	month = may,
	year = {2018},
	note = {arXiv: 1805.12185},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1805.12185 PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/S6BJWVZT/Liu et al. - 2018 - Fine-Pruning Defending Against Backdooring Attack.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/P5BMDYAH/1805.html:text/html}
}

@article{gu_badnets:_2017,
	title = {{BadNets}: {Identifying} {Vulnerabilities} in the {Machine} {Learning} {Model} {Supply} {Chain}},
	shorttitle = {{BadNets}},
	abstract = {Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a {\textbackslash}emph\{BadNet\}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of \{25\}{\textbackslash}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.},
	urldate = {2019-08-20},
	journal = {arXiv:1708.06733 [cs]},
	author = {Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.06733},
	keywords = {Computer Science - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv\:1708.06733 PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/GVUUC58A/Gu et al. - 2017 - BadNets Identifying Vulnerabilities in the Machin.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/438JUZ8B/1708.html:text/html}
}

@article{wang_neural_2019,
	title = {Neural {Cleanse}: {Identifying} and {Mitigating} {Backdoor} {Attacks} in {Neural} {Networks}},
	abstract = {Lack of transparency in deep neural networks (DNNs) make them susceptible to backdoor attacks, where hidden associations or triggers override normal classiﬁcation to produce unexpected results. For example, a model with a backdoor always identiﬁes a face as Bill Gates if a speciﬁc symbol is present in the input. Backdoors can stay hidden indeﬁnitely until activated by an input, and present a serious security risk to many security or safety related applications, e.g., biometric authentication systems or self-driving cars.},
	language = {en},
	author = {Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
	year = {2019},
	pages = {17},
	file = {Wang et al. - Neural Cleanse Identifying and Mitigating Backdoo.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/XTQY8S69/Wang et al. - Neural Cleanse Identifying and Mitigating Backdoo.pdf:application/pdf}
}

@article{liu_trojaning_2017,
	title = {Trojaning {Attack} on {Neural} {Networks}},
	abstract = {With the fast spread of machine learning techniques, sharing and adopting public machine learning models become very popular. This gives attackers many new opportunities. In this paper, we propose a trojaning attack on neuron networks. As the models are not intuitive for human to understand, the attack features stealthiness. Deploying trojaned models can cause various severe consequences including endangering human lives (in applications like auto driving). We first inverse the neuron network to generate a general trojan trigger, and then retrain the model with external datasets to inject malicious behaviors to the model. The malicious behaviors are only activated by inputs stamped with the trojan trigger. In our attack, we do not need to tamper with the original training process, which usually takes weeks to months. Instead, it takes minutes to hours to apply our attack. Also, we do not require the datasets that are used to train the model. In practice, the datasets are usually not shared due to privacy or copyright concerns. We use five different applications to demonstrate the power of our attack, and perform a deep analysis on the possible factors that affect the attack. The results show that our attack is highly effective and efficient. The trojaned behaviors can be successfully triggered (with nearly 100\% possibility) without affecting its test accuracy for normal input data. Also, it only takes a small amount of time to attack a complex neuron network model. In the end, we also discuss possible defense against such attacks.},
	language = {en},
	author = {Liu, Yingqi and Ma, Shiqing and Aafer, Yousra},
	year = {2017},
	pages = {17},
	file = {Liu et al. - Trojaning Attack on Neural Networks.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/5RDWTI2N/Liu et al. - Trojaning Attack on Neural Networks.pdf:application/pdf}
}

@inproceedings{biggio_bagging_2011,
	address = {Naples, Italy},
	title = {Bagging classifiers for fighting poisoning attacks in adversarial environments},
	abstract = {Abstract. Pattern recognition systems have been widely used in ad-versarial classification tasks like spam filtering and intrusion detection in computer networks. In these applications a malicious adversary may successfully mislead a classifier by “poisoning ” its training data with carefully designed attacks. Bagging is a well-known ensemble construc-tion method, where each classifier in the ensemble is trained on a different bootstrap replicate of the training set. Recent work has shown that bag-ging can reduce the influence of outliers in training data, especially if the most outlying observations are resampled with a lower probability. In this work we argue that poisoning attacks can be viewed as a particu-lar category of outliers, and, thus, bagging ensembles may be effectively exploited against them. We experimentally assess the effectiveness of bagging on a real, widely used spam filter, and on a web-based intrusion detection system. Our preliminary results suggest that bagging ensem-bles can be a very promising defence strategy against poisoning attacks, and give us valuable insights for future research work. 1},
	booktitle = {10th {Int}’l {Workshop} on {Multiple} {Classifier} {Systems}, volume 6713 of {LNCS}},
	publisher = {Springer},
	author = {Biggio, Battista and Corona, Igino and Fumera, Giorgio and Giacinto, Giorgio and Roli, Fabio},
	year = {2011},
	pages = {350--359},
	file = {Citeseer - Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/XFMDC428/Biggio et al. - Bagging classifiers for fighting poisoning attacks.pdf:application/pdf;Citeseer - Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/YHBGBQDC/summary.html:text/html}
}

@inproceedings{moustafa_unsw-nb15:_2015,
	title = {{UNSW}-{NB}15: a comprehensive data set for network intrusion detection systems ({UNSW}-{NB}15 network data set)},
	shorttitle = {{UNSW}-{NB}15},
	abstract = {One of the major research challenges in this field is the unavailability of a comprehensive network based data set which can reflect modern network traffic scenarios, vast varieties of low footprint intrusions and depth structured information about the network traffic. Evaluating network intrusion detection systems research efforts, KDD98, KDDCUP99 and NSLKDD benchmark data sets were generated a decade ago. However, numerous current studies showed that for the current network threat environment, these data sets do not inclusively reflect network traffic and modern low footprint attacks. Countering the unavailability of network benchmark data set challenges, this paper examines a UNSW-NB15 data set creation. This data set has a hybrid of the real modern normal and the contemporary synthesized attack activities of the network traffic. Existing and novel methods are utilised to generate the features of the UNSWNB15 data set. This data set is available for research purposes and can be accessed from the link.},
	booktitle = {2015 {Military} {Communications} and {Information} {Systems} {Conference} ({MilCIS})},
	author = {Moustafa, Nour and Slay, Jill},
	month = nov,
	year = {2015},
	keywords = {telecommunication traffic, Feature extraction, computer network security, IP networks, Servers, Benchmark testing, Data models, low footprint attacks, network intrusion detection systems, network traffic, NIDS, pcap files, Telecommunication traffic, testbed, Training, UNSW-NB15 data set, UNSW-NB15 network data set},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/8IV9KVPW/7348942.html:text/html;IEEE Xplore Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/YTT634J3/Moustafa und Slay - 2015 - UNSW-NB15 a comprehensive data set for network in.pdf:application/pdf}
}

@article{williams_preliminary_2006,
	title = {A {Preliminary} {Performance} {Comparison} of {Five} {Machine} {Learning} {Algorithms} for {Practical} {IP} {Traffic} {Flow} {Classification}},
	volume = {36},
	abstract = {Loading...},
	number = {5},
	urldate = {2019-09-08},
	journal = {SIGCOMM Comput. Commun. Rev.},
	author = {Williams, Nigel and Zander, Sebastian and Armitage, Grenville},
	month = oct,
	year = {2006},
	keywords = {machine learning, traffic classification},
	pages = {5--16},
	file = {ACM Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/FTR3TPF9/Williams et al. - 2006 - A Preliminary Performance Comparison of Five Machi.pdf:application/pdf}
}

@article{friedman_greedy_2001,
	title = {Greedy {Function} {Approximation}: {A} {Gradient} {Boosting} {Machine}},
	volume = {29},
	shorttitle = {Greedy {Function} {Approximation}},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	number = {5},
	urldate = {2019-09-08},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	year = {2001},
	pages = {1189--1232}
}

@article{gharib_evaluation_2016,
	title = {An {Evaluation} {Framework} for {Intrusion} {Detection} {Dataset}},
	abstract = {The growing number of security threats on the Internet and computer networks demands highly reliable security solutions. Meanwhile, Intrusion Detection (IDSs) and Intrusion Prevention Systems (IPSs) have an important role in the design and development of a robust network infrastructure that can defend computer networks by detecting and blocking a variety of attacks. Reliable benchmark datasets are critical to test and evaluate the performance of a detection system. There exist a number of such datasets, for example, DARPA98, KDD99, ISC2012, and ADFA13 that have been used by the researchers to evaluate the performance of their intrusion detection and prevention approaches. However, not enough research has focused on the evaluation and assessment of the datasets themselves. In this paper we present a comprehensive evaluation of the existing datasets using our proposed criteria, and propose an evaluation framework for IDS and IPS datasets.},
	journal = {2016 International Conference on Information Science and Security (ICISS)},
	author = {Gharib, Amirhossein and Sharafaldin, Iman and Lashkari, Arash Habibi and Ghorbani, Ali A.},
	year = {2016},
	keywords = {Internet, Benchmark (computing), Blocking (computing), Coefficient, computer network, Dental Intrusion, Intrusion detection system, Sensor, Silo (dataset)},
	pages = {1--6}
}

@misc{vormayr_go-flows_2019,
	title = {go-flows},
	copyright = {LGPL-3.0},
	url = {https://github.com/CN-TU/go-flows},
	abstract = {Flow Exporter implementation in go. CN contact: GV},
	urldate = {2019-09-08},
	publisher = {CN Group, Institute of Telecommunications, TU Wien},
	author = {Vormayr, Gernot},
	month = aug,
	year = {2019},
	note = {Commit 0816e6}
}

@article{esposito_comparative_1997,
	title = {A comparative analysis of methods for pruning decision trees},
	volume = {19},
	abstract = {In this paper, we address the problem of retrospectively pruning decision trees induced from data, according to a top-down approach. This problem has received considerable attention in the areas of pattern recognition and machine learning, and many distinct methods have been proposed in literature. We make a comparative study of six well-known pruning methods with the aim of understanding their theoretical foundations, their computational complexity, and the strengths and weaknesses of their formulation. Comments on the characteristics of each method are empirically supported. In particular, a wide experimentation performed on several data sets leads us to opposite conclusions on the predictive accuracy of simplified trees from some drawn in the literature. We attribute this divergence to differences in experimental designs. Finally, we prove and make use of a property of the reduced error pruning method to obtain an objective evaluation of the tendency to overprune/underprune observed in each method.},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Esposito, Floriana and Malerba, Donato and Semeraro, Giovanni and Kay, J.},
	month = may,
	year = {1997},
	keywords = {machine learning, Machine learning, Testing, Accuracy, Classification tree analysis, computational complexity, Computational complexity, decision theory, decision tree pruning, Decision trees, Design for experiments, grafting operators, learning systems, Medical diagnosis, optimisation, Pattern recognition, reduced error pruning, Regression tree analysis, top-down induction, trees (mathematics)},
	pages = {476--491},
	file = {IEEE Xplore Abstract Record:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/FICUK9W3/589207.html:text/html;IEEE Xplore Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/WAR5839B/Esposito et al. - 1997 - A comparative analysis of methods for pruning deci.pdf:application/pdf}
}

@article{karnin_simple_1990,
	title = {A simple procedure for pruning back-propagation trained neural networks},
	volume = {1},
	abstract = {The sensitivity of the global error (cost) function to the inclusion/exclusion of each synapse in the artificial neural network is estimated. Introduced are shadow arrays which keep track of the incremental changes to the synaptic weights during a single pass of back-propagating learning. The synapses are then ordered by decreasing sensitivity numbers so that the network can be efficiently pruned by discarding the last items of the sorted list. Unlike previous approaches, this simple procedure does not require a modification of the cost function, does not interfere with the learning process, and demands a negligible computational overhead.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Karnin, E. D.},
	month = jun,
	year = {1990},
	keywords = {learning systems, Artificial neural networks, back-propagation, Cities and towns, Computational efficiency, Computer networks, cost function, Cost function, global error, learning process, Learning systems, Logistics, neural nets, neural networks, Neural networks, Neurons, sensitivity, shadow arrays, synaptic weights, Training data},
	pages = {239--242},
	file = {IEEE Xplore Abstract Record:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/7PV4W8J7/80236.html:text/html;IEEE Xplore Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/YHHGXDSY/Karnin - 1990 - A simple procedure for pruning back-propagation tr.pdf:application/pdf}
}

@inproceedings{sietsma_neural_1988,
	address = {San Diego, CA},
	title = {Neural net pruning-why and how},
	booktitle = {Proceedings of the {International} {Conference} on {Neural} {Networks}},
	publisher = {IEEE},
	author = {Sietsma, Jocelyn},
	year = {1988},
	pages = {325--333}
}

@inproceedings{chen_robust_2019,
	address = {Long Beach, CA},
	title = {Robust {Decision} {Trees} {Against} {Adversarial} {Examples}},
	abstract = {Although adversarial examples and model robustness have been extensively studied in the context of linear models and neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited. In this paper, we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. At its core, our method aims to optimize the performance under the worstcase perturbation of input features, which leads to a max-min saddle point problem. Incorporating this saddle point objective into the decision tree building procedure is non-trivial due to the discrete nature of trees—a naive approach to ﬁnding the best split according to this saddle point objective will take exponential time. To make our approach practical and scalable, we propose efﬁcient tree building algorithms by approximating the inner minimizer in this saddle point problem, and present efﬁcient implementations for classical information gain based trees as well as state-of-the-art tree boosting models such as XGBoost. Experimental results on real world datasets demonstrate that the proposed algorithms can substantially improve the robustness of tree-based models against adversarial examples.},
	language = {en},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Hongge and Zhang, Huan and Boning, Duane and Hsieh, Cho-Jui},
	year = {2019},
	pages = {1122--1131},
	file = {Chen et al. - Robust Decision Trees Against Adversarial Examples.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/7WHJJ8DY/Chen et al. - Robust Decision Trees Against Adversarial Examples.pdf:application/pdf}
}

@inproceedings{russu_secure_2016,
	address = {Vienna, Austria},
	title = {Secure {Kernel} {Machines} against {Evasion} {Attacks}},
	isbn = {978-1-4503-4573-6},
	abstract = {Machine learning is widely used in security-sensitive settings like spam and malware detection, although it has been shown that malicious data can be carefully modiﬁed at test time to evade detection. To overcome this limitation, adversaryaware learning algorithms have been developed, exploiting robust optimization and game-theoretical models to incorporate knowledge of potential adversarial data manipulations into the learning algorithm. Despite these techniques have been shown to be eﬀective in some adversarial learning tasks, their adoption in practice is hindered by diﬀerent factors, including the diﬃculty of meeting speciﬁc theoretical requirements, the complexity of implementation, and scalability issues, in terms of computational time and space required during training. In this work, we aim to develop secure kernel machines against evasion attacks that are not computationally more demanding than their non-secure counterparts. In particular, leveraging recent work on robustness and regularization, we show that the security of a linear classiﬁer can be drastically improved by selecting a proper regularizer, depending on the kind of evasion attack, as well as unbalancing the cost of classiﬁcation errors. We then discuss the security of nonlinear kernel machines, and show that a proper choice of the kernel function is crucial. We also show that unbalancing the cost of classiﬁcation errors and varying some kernel parameters can further improve classiﬁer security, yielding decision functions that better enclose the legitimate data. Our results on spam and PDF malware detection corroborate our analysis.},
	language = {en},
	urldate = {2019-09-09},
	booktitle = {Proceedings of the 2016 {ACM} {Workshop} on {Artificial} {Intelligence} and {Security} - {ALSec} '16},
	publisher = {ACM Press},
	author = {Russu, Paolo and Demontis, Ambra and Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
	year = {2016},
	pages = {59--69},
	file = {Russu et al. - 2016 - Secure Kernel Machines against Evasion Attacks.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/G4XIDKGA/Russu et al. - 2016 - Secure Kernel Machines against Evasion Attacks.pdf:application/pdf}
}

@incollection{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	urldate = {2019-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {MIT Press},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	year = {2014},
	pages = {3320--3328},
	file = {NIPS Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/T56ZNBZ8/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf:application/pdf;NIPS Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/CGKESR56/5347-how-transferable-are-features-in-deep-neural-networks.html:text/html}
}

@inproceedings{sharafaldin_toward_2018,
	address = {Funchal, Madeira, Portugal},
	title = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}},
	shorttitle = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}},
	abstract = {Intrusion Detection, IDS Dataset, DoS, Web Attack, Inﬁltration, Brute Force.},
	language = {en},
	urldate = {2019-09-10},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Information} {Systems} {Security} and {Privacy}},
	publisher = {SCITEPRESS},
	author = {Sharafaldin, Iman and Habibi Lashkari, Arash and Ghorbani, Ali A.},
	year = {2018},
	pages = {108--116},
	file = {Sharafaldin et al. - 2018 - Toward Generating a New Intrusion Detection Datase.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/H2PGUEGW/Sharafaldin et al. - 2018 - Toward Generating a New Intrusion Detection Datase.pdf:application/pdf}
}

@article{apley_visualizing_2016,
	title = {Visualizing the {Effects} of {Predictor} {Variables} in {Black} {Box} {Supervised} {Learning} {Models}},
	abstract = {When fitting black box supervised learning models (e.g., complex trees, neural networks, boosted trees, random forests, nearest neighbors, local kernel-weighted methods, etc.), visualizing the main effects of the individual predictor variables and their low-order interaction effects is often important, and partial dependence (PD) plots are the most popular approach for accomplishing this. However, PD plots involve a serious pitfall if the predictor variables are far from independent, which is quite common with large observational data sets. Namely, PD plots require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data, which can render the PD plots unreliable. Although marginal plots (M plots) do not require such extrapolation, they produce substantially biased and misleading results when the predictors are dependent, analogous to the omitted variable bias in regression. We present a new visualization approach that we term accumulated local effects (ALE) plots, which inherits the desirable characteristics of PD and M plots, without inheriting their preceding shortcomings. Like M plots, ALE plots do not require extrapolation; and like PD plots, they are not biased by the omitted variable phenomenon. Moreover, ALE plots are far less computationally expensive than PD plots.},
	urldate = {2019-09-13},
	journal = {arXiv:1612.08468 [stat]},
	author = {Apley, Daniel W. and Zhu, Jingyu},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.08468},
	keywords = {Statistics - Methodology},
	annote = {Comment: The R package ALEPlot is available on CRAN. The new version contains refined definitions of ALE effects, a new illustrative example, theorems and proofs of asymptotic properties of ALE effects and estimators, and extra implementation details},
	file = {arXiv\:1612.08468 PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/C9NVBZDE/Apley und Zhu - 2016 - Visualizing the Effects of Predictor Variables in .pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/KUE3BBVZ/1612.html:text/html}
}

@article{pedregosa_scikit-learn:_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	issn = {1533-7928},
	shorttitle = {Scikit-learn},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language.  Emphasis is put on ease of use, performance, documentation, and API consistency.  It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings.  Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	urldate = {2019-09-16},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and others},
	month = oct,
	year = {2011},
	pages = {2825--2830},
	file = {Fulltext PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/UFKVHHSL/Pedregosa et al. - 2011 - Scikit-learn Machine Learning in Python.pdf:application/pdf}
}

@article{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and others},
	year = {2017},
	pages = {4},
	file = {Paszke et al. - Automatic differentiation in PyTorch.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/X42BEL2Z/Paszke et al. - Automatic differentiation in PyTorch.pdf:application/pdf}
}

@misc{wikipedia_convolutional_2019,
	title = {Convolutional neural network},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&oldid=921208341},
	abstract = {In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of deep neural networks, most commonly applied to analyzing visual imagery.
CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The "fully-connectedness" of these networks makes them prone to overfitting data. Typical ways of regularization include adding some form of magnitude measurement of weights to the loss function. However, CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble more complex patterns using smaller and simpler patterns. Therefore, on the scale of connectedness and complexity, CNNs are on the lower extreme.
They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on their shared-weights architecture and translation invariance characteristics.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns the filters that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.

They have applications in image and video recognition, recommender systems, image classification, medical image analysis, and natural language processing.},
	language = {en},
	urldate = {2019-10-15},
	journal = {Wikipedia},
	author = {{Wikipedia}},
	month = oct,
	year = {2019},
	note = {Page Version ID: 921208341},
	file = {Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/3VMLXM9B/index.html:text/html}
}

@article{paszke_automatic_2017-1,
	title = {Automatic differentiation in {PyTorch}},
	abstract = {In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.},
	language = {en},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
	pages = {4},
	file = {Paszke et al. - Automatic differentiation in PyTorch.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/SHSISEQR/Paszke et al. - Automatic differentiation in PyTorch.pdf:application/pdf}
}

@article{pedregosa_scikit-learn:_2011-1,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simpliﬁed BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
	language = {en},
	journal = {MACHINE LEARNING IN PYTHON},
	author = {Pedregosa, Fabian and Varoquaux, Gael and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David},
	year = {2011},
	pages = {6},
	file = {Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/QXDG2E3R/Pedregosa et al. - Scikit-learn Machine Learning in Python.pdf:application/pdf}
}

@inproceedings{erlacher_how_2018,
	address = {New York, NY, USA},
	series = {{WTMC} '18},
	title = {How to {Test} an {IDS}?: {GENESIDS}: {An} {Automated} {System} for {Generating} {Attack} {Traffic}},
	isbn = {978-1-4503-5910-8},
	shorttitle = {How to {Test} an {IDS}?},
	url = {http://doi.acm.org/10.1145/3229598.3229601},
	doi = {10.1145/3229598.3229601},
	abstract = {Evaluating the attack coverage of signature-based Network Intrusion Detection System (NIDS) is a necessary but difficult task. Often, live or recorded real-world traffic is used. However, firstly, real-world network traffic is hard to come by at larger scale and the few available traces usually do not contain application layer payload. Secondly and more importantly, it contains only very few realistic attacks. So, the question remains how to test a NIDS? We propose GENESIDS, a system that automatically generates user definable HTTP attacks and, thus, allows for straightforward creation of network traces (or live traffic) where the number of different detectable events is only confined by the given attack definitions. By using an input format that follows the Snort syntax, the system can take advantage of thousands of realistic attack definitions. Our system can be used in combination with traffic generators to maintain typical load patterns as background traffic. Our evaluation shows that GENESIDS is able to reliably produce a very broad variation of HTTP attacks. GENESIDS is available as Open Source softtware.},
	urldate = {2019-10-15},
	booktitle = {Proceedings of the 2018 {Workshop} on {Traffic} {Measurements} for {Cybersecurity}},
	publisher = {ACM},
	author = {Erlacher, Felix and Dressler, Falko},
	year = {2018},
	note = {event-place: Budapest, Hungary},
	pages = {46--51},
	file = {ACM Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/ZANFRZPG/Erlacher and Dressler - 2018 - How to Test an IDS GENESIDS An Automated System.pdf:application/pdf}
}