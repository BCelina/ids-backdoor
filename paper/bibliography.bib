
@article{radford_network_2018,
	title = {Network {Traffic} {Anomaly} {Detection} {Using} {Recurrent} {Neural} {Networks}},
	abstract = {We show that a recurrent neural network is able to learn a model to represent sequences of communications between computers on a network and can be used to identify outlier network traffic. Defending computer networks is a challenging problem and is typically addressed by manually identifying known malicious actor behavior and then specifying rules to recognize such behavior in network communications. However, these rule-based approaches often generalize poorly and identify only those patterns that are already known to researchers. An alternative approach that does not rely on known malicious behavior patterns can potentially also detect previously unseen patterns. We tokenize and compress netflow into sequences of "words" that form "sentences" representative of a conversation between computers. These sentences are then used to generate a model that learns the semantic and syntactic grammar of the newly generated language. We use Long-Short-Term Memory (LSTM) cell Recurrent Neural Networks (RNN) to capture the complex relationships and nuances of this language. The language model is then used predict the communications between two IPs and the prediction error is used as a measurement of how typical or atyptical the observed communication are. By learning a model that is specific to each network, yet generalized to typical computer-to-computer traffic within and outside the network, a language model is able to identify sequences of network activity that are outliers with respect to the model. We demonstrate positive unsupervised attack identification performance (AUC 0.84) on the ISCX IDS dataset which contains seven days of network activity with normal traffic and four distinct attack patterns.},
	urldate = {2019-07-30},
	journal = {arXiv:1803.10769 [cs]},
	author = {Radford, Benjamin J. and Apolonio, Leonardo M. and Trias, Antonio J. and Simpson, Jim A.},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.10769},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	annote = {Comment: Prepared for the 2017 National Symposium on Sensor and Data Fusion},
	file = {arXiv\:1803.10769 PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/XMB4GLA5/Radford et al. - 2018 - Network Traffic Anomaly Detection Using Recurrent .pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/QJ9R9SV7/1803.html:text/html}
}

@article{meghdouri_analysis_2018,
	title = {Analysis of {Lightweight} {Feature} {Vectors} for {Attack} {Detection} in {Network} {Traffic}},
	volume = {8},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	abstract = {The consolidation of encryption and big data in network communications have made deep packet inspection no longer feasible in large networks. Early attack detection requires feature vectors which are easy to extract, process, and analyze, allowing their generation also from encrypted traffic. So far, experts have selected features based on their intuition, previous research, or acritically assuming standards, but there is no general agreement about the features to use for attack detection in a broad scope. We compared five lightweight feature sets that have been proposed in the scientific literature for the last few years, and evaluated them with supervised machine learning. For our experiments, we use the UNSW-NB15 dataset, recently published as a new benchmark for network security. Results showed three remarkable findings: (1) Analysis based on source behavior instead of classic flow profiles is more effective for attack detection; (2) meta-studies on past research can be used to establish satisfactory benchmarks; and (3) features based on packet length are clearly determinant for capturing malicious activity. Our research showed that vectors currently used for attack detection are oversized, their accuracy and speed can be improved, and are to be adapted for dealing with encrypted traffic.},
	language = {en},
	number = {11},
	urldate = {2019-07-30},
	journal = {Applied Sciences},
	author = {Meghdouri, Fares and Zseby, Tanja and Iglesias, Félix},
	month = nov,
	year = {2018},
	keywords = {feature selection, network attack detection, supervised learning},
	pages = {2196},
	file = {Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/DLBHCM7Q/Meghdouri et al. - 2018 - Analysis of Lightweight Feature Vectors for Attack.pdf:application/pdf;Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/EFNANYEY/2196.html:text/html}
}

@article{liu_fine-pruning:_2018,
	title = {Fine-{Pruning}: {Defending} {Against} {Backdooring} {Attacks} on {Deep} {Neural} {Networks}},
	shorttitle = {Fine-{Pruning}},
	abstract = {Deep neural networks (DNNs) provide excellent performance across a wide range of classification tasks, but their training requires high computational resources and is often outsourced to third parties. Recent work has shown that outsourced training introduces the risk that a malicious trainer will return a backdoored DNN that behaves normally on most inputs but causes targeted misclassifications or degrades the accuracy of the network when a trigger known only to the attacker is present. In this paper, we provide the first effective defenses against backdoor attacks on DNNs. We implement three backdoor attacks from prior work and use them to investigate two promising defenses, pruning and fine-tuning. We show that neither, by itself, is sufficient to defend against sophisticated attackers. We then evaluate fine-pruning, a combination of pruning and fine-tuning, and show that it successfully weakens or even eliminates the backdoors, i.e., in some cases reducing the attack success rate to 0\% with only a 0.4\% drop in accuracy for clean (non-triggering) inputs. Our work provides the first step toward defenses against backdoor attacks in deep neural networks.},
	urldate = {2019-08-20},
	journal = {arXiv:1805.12185 [cs]},
	author = {Liu, Kang and Dolan-Gavitt, Brendan and Garg, Siddharth},
	month = may,
	year = {2018},
	note = {arXiv: 1805.12185},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv\:1805.12185 PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/S6BJWVZT/Liu et al. - 2018 - Fine-Pruning Defending Against Backdooring Attack.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/P5BMDYAH/1805.html:text/html}
}

@article{gu_badnets:_2017,
	title = {{BadNets}: {Identifying} {Vulnerabilities} in the {Machine} {Learning} {Model} {Supply} {Chain}},
	shorttitle = {{BadNets}},
	abstract = {Deep learning-based techniques have achieved state-of-the-art performance on a wide variety of recognition and classification tasks. However, these networks are typically computationally expensive to train, requiring weeks of computation on many GPUs; as a result, many users outsource the training procedure to the cloud or rely on pre-trained models that are then fine-tuned for a specific task. In this paper we show that outsourced training introduces new security risks: an adversary can create a maliciously trained network (a backdoored neural network, or a {\textbackslash}emph\{BadNet\}) that has state-of-the-art performance on the user's training and validation samples, but behaves badly on specific attacker-chosen inputs. We first explore the properties of BadNets in a toy example, by creating a backdoored handwritten digit classifier. Next, we demonstrate backdoors in a more realistic scenario by creating a U.S. street sign classifier that identifies stop signs as speed limits when a special sticker is added to the stop sign; we then show in addition that the backdoor in our US street sign detector can persist even if the network is later retrained for another task and cause a drop in accuracy of \{25\}{\textbackslash}\% on average when the backdoor trigger is present. These results demonstrate that backdoors in neural networks are both powerful and---because the behavior of neural networks is difficult to explicate---stealthy. This work provides motivation for further research into techniques for verifying and inspecting neural networks, just as we have developed tools for verifying and debugging software.},
	urldate = {2019-08-20},
	journal = {arXiv:1708.06733 [cs]},
	author = {Gu, Tianyu and Dolan-Gavitt, Brendan and Garg, Siddharth},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.06733},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {arXiv\:1708.06733 PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/GVUUC58A/Gu et al. - 2017 - BadNets Identifying Vulnerabilities in the Machin.pdf:application/pdf;arXiv.org Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/438JUZ8B/1708.html:text/html}
}

@article{wang_neural_2019,
	title = {Neural {Cleanse}: {Identifying} and {Mitigating} {Backdoor} {Attacks} in {Neural} {Networks}},
	abstract = {Lack of transparency in deep neural networks (DNNs) make them susceptible to backdoor attacks, where hidden associations or triggers override normal classiﬁcation to produce unexpected results. For example, a model with a backdoor always identiﬁes a face as Bill Gates if a speciﬁc symbol is present in the input. Backdoors can stay hidden indeﬁnitely until activated by an input, and present a serious security risk to many security or safety related applications, e.g., biometric authentication systems or self-driving cars.},
	language = {en},
	author = {Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y},
	year = {2019},
	pages = {17},
	file = {Wang et al. - Neural Cleanse Identifying and Mitigating Backdoo.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/XTQY8S69/Wang et al. - Neural Cleanse Identifying and Mitigating Backdoo.pdf:application/pdf}
}

@article{liu_trojaning_2017,
	title = {Trojaning {Attack} on {Neural} {Networks}},
	abstract = {With the fast spread of machine learning techniques, sharing and adopting public machine learning models become very popular. This gives attackers many new opportunities. In this paper, we propose a trojaning attack on neuron networks. As the models are not intuitive for human to understand, the attack features stealthiness. Deploying trojaned models can cause various severe consequences including endangering human lives (in applications like auto driving). We first inverse the neuron network to generate a general trojan trigger, and then retrain the model with external datasets to inject malicious behaviors to the model. The malicious behaviors are only activated by inputs stamped with the trojan trigger. In our attack, we do not need to tamper with the original training process, which usually takes weeks to months. Instead, it takes minutes to hours to apply our attack. Also, we do not require the datasets that are used to train the model. In practice, the datasets are usually not shared due to privacy or copyright concerns. We use five different applications to demonstrate the power of our attack, and perform a deep analysis on the possible factors that affect the attack. The results show that our attack is highly effective and efficient. The trojaned behaviors can be successfully triggered (with nearly 100\% possibility) without affecting its test accuracy for normal input data. Also, it only takes a small amount of time to attack a complex neuron network model. In the end, we also discuss possible defense against such attacks.},
	language = {en},
	author = {Liu, Yingqi and Ma, Shiqing and Aafer, Yousra},
	year = {2017},
	pages = {17},
	file = {Liu et al. - Trojaning Attack on Neural Networks.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/5RDWTI2N/Liu et al. - Trojaning Attack on Neural Networks.pdf:application/pdf}
}

@inproceedings{biggio_bagging_2011,
	title = {Bagging classifiers for fighting poisoning attacks in adversarial environments},
	abstract = {Abstract. Pattern recognition systems have been widely used in ad-versarial classification tasks like spam filtering and intrusion detection in computer networks. In these applications a malicious adversary may successfully mislead a classifier by “poisoning ” its training data with carefully designed attacks. Bagging is a well-known ensemble construc-tion method, where each classifier in the ensemble is trained on a different bootstrap replicate of the training set. Recent work has shown that bag-ging can reduce the influence of outliers in training data, especially if the most outlying observations are resampled with a lower probability. In this work we argue that poisoning attacks can be viewed as a particu-lar category of outliers, and, thus, bagging ensembles may be effectively exploited against them. We experimentally assess the effectiveness of bagging on a real, widely used spam filter, and on a web-based intrusion detection system. Our preliminary results suggest that bagging ensem-bles can be a very promising defence strategy against poisoning attacks, and give us valuable insights for future research work. 1},
	booktitle = {Roli ({Eds}.), 10th {Int}’l {Workshop} on {Multiple} {Classifier} {Systems}, volume 6713 of {LNCS}, {Springer}-{Verlag}},
	author = {Biggio, Battista and Corona, Igino and Fumera, Giorgio and Giacinto, Giorgio and Roli, Fabio},
	year = {2011},
	pages = {350--359},
	file = {Citeseer - Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/XFMDC428/Biggio et al. - Bagging classifiers for fighting poisoning attacks.pdf:application/pdf;Citeseer - Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/YHBGBQDC/summary.html:text/html}
}

@inproceedings{gharib_evaluation_2016,
	title = {An {Evaluation} {Framework} for {Intrusion} {Detection} {Dataset}},
	abstract = {The growing number of security threats on the Internet and computer networks demands highly reliable security solutions. Meanwhile, Intrusion Detection (IDSs) and Intrusion Prevention Systems (IPSs) have an important role in the design and development of a robust network infrastructure that can defend computer networks by detecting and blocking a variety of attacks. Reliable benchmark datasets are critical to test and evaluate the performance of a detection system. There exist a number of such datasets, for example, DARPA98, KDD99, ISC2012, and ADFA13 that have been used by the researchers to evaluate the performance of their intrusion detection and prevention approaches. However, not enough research has focused on the evaluation and assessment of the datasets themselves. In this paper we present a comprehensive evaluation of the existing datasets using our proposed criteria, and propose an evaluation framework for IDS and IPS datasets.},
	booktitle = {2016 {International} {Conference} on {Information} {Science} and {Security} ({ICISS})},
	author = {Gharib, A. and Sharafaldin, I. and Lashkari, A. H. and Ghorbani, A. A.},
	month = dec,
	year = {2016},
	keywords = {Protocols, Internet, IP networks, security of data, Computer network reliability, computer networks, IDS, Intrusion detection, intrusion detection dataset, intrusion prevention system, IPS, Payloads, Reliability, Testing},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/CLN8DWGB/7885840.html:text/html;IEEE Xplore Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/2DB8HI2G/Gharib et al. - 2016 - An Evaluation Framework for Intrusion Detection Da.pdf:application/pdf}
}

@inproceedings{moustafa_unsw-nb15:_2015,
	title = {{UNSW}-{NB}15: a comprehensive data set for network intrusion detection systems ({UNSW}-{NB}15 network data set)},
	shorttitle = {{UNSW}-{NB}15},
	abstract = {One of the major research challenges in this field is the unavailability of a comprehensive network based data set which can reflect modern network traffic scenarios, vast varieties of low footprint intrusions and depth structured information about the network traffic. Evaluating network intrusion detection systems research efforts, KDD98, KDDCUP99 and NSLKDD benchmark data sets were generated a decade ago. However, numerous current studies showed that for the current network threat environment, these data sets do not inclusively reflect network traffic and modern low footprint attacks. Countering the unavailability of network benchmark data set challenges, this paper examines a UNSW-NB15 data set creation. This data set has a hybrid of the real modern normal and the contemporary synthesized attack activities of the network traffic. Existing and novel methods are utilised to generate the features of the UNSWNB15 data set. This data set is available for research purposes and can be accessed from the link.},
	booktitle = {2015 {Military} {Communications} and {Information} {Systems} {Conference} ({MilCIS})},
	author = {Moustafa, N. and Slay, J.},
	month = nov,
	year = {2015},
	keywords = {telecommunication traffic, Feature extraction, computer network security, IP networks, Servers, Benchmark testing, Data models, low footprint attacks, network intrusion detection systems, network traffic, NIDS, pcap files, Telecommunication traffic, testbed, Training, UNSW-NB15 data set, UNSW-NB15 network data set},
	pages = {1--6},
	file = {IEEE Xplore Abstract Record:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/8IV9KVPW/7348942.html:text/html;IEEE Xplore Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/YTT634J3/Moustafa und Slay - 2015 - UNSW-NB15 a comprehensive data set for network in.pdf:application/pdf}
}

@article{williams_preliminary_2006,
	title = {A {Preliminary} {Performance} {Comparison} of {Five} {Machine} {Learning} {Algorithms} for {Practical} {IP} {Traffic} {Flow} {Classification}},
	volume = {36},
	abstract = {Loading...},
	number = {5},
	urldate = {2019-09-08},
	journal = {SIGCOMM Comput. Commun. Rev.},
	author = {Williams, Nigel and Zander, Sebastian and Armitage, Grenville},
	month = oct,
	year = {2006},
	keywords = {machine learning, traffic classification},
	pages = {5--16},
	file = {ACM Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/FTR3TPF9/Williams et al. - 2006 - A Preliminary Performance Comparison of Five Machi.pdf:application/pdf}
}

@article{friedman_greedy_2001,
	title = {Greedy {Function} {Approximation}: {A} {Gradient} {Boosting} {Machine}},
	volume = {29},
	shorttitle = {Greedy {Function} {Approximation}},
	abstract = {Function estimation/approximation is viewed from the perspective of numerical optimization in function space, rather than parameter space. A connection is made between stagewise additive expansions and steepest-descent minimization. A general gradient descent "boosting" paradigm is developed for additive expansions based on any fitting criterion. Specific algorithms are presented for least-squares, least absolute deviation, and Huber-M loss functions for regression, and multiclass logistic likelihood for classification. Special enhancements are derived for the particular case where the individual additive components are regression trees, and tools for interpreting such "TreeBoost" models are presented. Gradient boosting of regression trees produces competitive, highly robust, interpretable procedures for both regression and classification, especially appropriate for mining less than clean data. Connections between this approach and the boosting methods of Freund and Shapire and Friedman, Hastie and Tibshirani are discussed.},
	number = {5},
	urldate = {2019-09-08},
	journal = {The Annals of Statistics},
	author = {Friedman, Jerome H.},
	year = {2001},
	pages = {1189--1232}
}

@article{gharib_evaluation_2016-1,
	title = {An {Evaluation} {Framework} for {Intrusion} {Detection} {Dataset}},
	abstract = {The growing number of security threats on the Internet and computer networks demands highly reliable security solutions. Meanwhile, Intrusion Detection (IDSs) and Intrusion Prevention Systems (IPSs) have an important role in the design and development of a robust network infrastructure that can defend computer networks by detecting and blocking a variety of attacks. Reliable benchmark datasets are critical to test and evaluate the performance of a detection system. There exist a number of such datasets, for example, DARPA98, KDD99, ISC2012, and ADFA13 that have been used by the researchers to evaluate the performance of their intrusion detection and prevention approaches. However, not enough research has focused on the evaluation and assessment of the datasets themselves. In this paper we present a comprehensive evaluation of the existing datasets using our proposed criteria, and propose an evaluation framework for IDS and IPS datasets.},
	journal = {2016 International Conference on Information Science and Security (ICISS)},
	author = {Gharib, Amirhossein and Sharafaldin, Iman and Lashkari, Arash Habibi and Ghorbani, Ali A.},
	year = {2016},
	keywords = {Benchmark (computing), Blocking (computing), Coefficient, computer network, Dental Intrusion, Internet, Intrusion detection system, Sensor, Silo (dataset)},
	pages = {1--6}
}

@misc{vormayr_cn-tu/go-flows_2019,
	title = {{CN}-{TU}/go-flows},
	copyright = {LGPL-3.0},
	url = {https://github.com/CN-TU/go-flows},
	abstract = {Flow Exporter implementation in go. CN contact: GV},
	urldate = {2019-09-08},
	publisher = {CN Group, Institute of Telecommunications, TU Wien},
	author = {Vormayr, Gernot},
	month = aug,
	year = {2019},
	note = {original-date: 2018-01-23T13:18:13Z}
}

@article{esposito_comparative_1997,
	title = {A comparative analysis of methods for pruning decision trees},
	volume = {19},
	abstract = {In this paper, we address the problem of retrospectively pruning decision trees induced from data, according to a top-down approach. This problem has received considerable attention in the areas of pattern recognition and machine learning, and many distinct methods have been proposed in literature. We make a comparative study of six well-known pruning methods with the aim of understanding their theoretical foundations, their computational complexity, and the strengths and weaknesses of their formulation. Comments on the characteristics of each method are empirically supported. In particular, a wide experimentation performed on several data sets leads us to opposite conclusions on the predictive accuracy of simplified trees from some drawn in the literature. We attribute this divergence to differences in experimental designs. Finally, we prove and make use of a property of the reduced error pruning method to obtain an objective evaluation of the tendency to overprune/underprune observed in each method.},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Esposito, F. and Malerba, D. and Semeraro, G. and Kay, J.},
	month = may,
	year = {1997},
	keywords = {Accuracy, Classification tree analysis, computational complexity, Computational complexity, decision theory, decision tree pruning, Decision trees, Design for experiments, grafting operators, learning systems, machine learning, Machine learning, Medical diagnosis, optimisation, Pattern recognition, reduced error pruning, Regression tree analysis, Testing, top-down induction, trees (mathematics)},
	pages = {476--491},
	file = {IEEE Xplore Abstract Record:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/FICUK9W3/589207.html:text/html;IEEE Xplore Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/WAR5839B/Esposito et al. - 1997 - A comparative analysis of methods for pruning deci.pdf:application/pdf}
}

@article{karnin_simple_1990,
	title = {A simple procedure for pruning back-propagation trained neural networks},
	volume = {1},
	abstract = {The sensitivity of the global error (cost) function to the inclusion/exclusion of each synapse in the artificial neural network is estimated. Introduced are shadow arrays which keep track of the incremental changes to the synaptic weights during a single pass of back-propagating learning. The synapses are then ordered by decreasing sensitivity numbers so that the network can be efficiently pruned by discarding the last items of the sorted list. Unlike previous approaches, this simple procedure does not require a modification of the cost function, does not interfere with the learning process, and demands a negligible computational overhead.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Karnin, E. D.},
	month = jun,
	year = {1990},
	keywords = {Artificial neural networks, back-propagation, Cities and towns, Computational efficiency, Computer networks, cost function, Cost function, global error, learning process, learning systems, Learning systems, Logistics, neural nets, neural networks, Neural networks, Neurons, sensitivity, shadow arrays, synaptic weights, Training data},
	pages = {239--242},
	file = {IEEE Xplore Abstract Record:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/7PV4W8J7/80236.html:text/html;IEEE Xplore Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/YHHGXDSY/Karnin - 1990 - A simple procedure for pruning back-propagation tr.pdf:application/pdf}
}

@inproceedings{sietsma_neural_1988,
	title = {Neural net pruning-why and how},
	volume = {1},
	booktitle = {Proceedings of {International} {Conference} on {Neural} {Networks}, {San} {Diego}, {CA}, 1988},
	author = {Sietsma, Jocelyn},
	year = {1988},
	pages = {325--333}
}

@article{chen_robust_2019,
	title = {Robust {Decision} {Trees} {Against} {Adversarial} {Examples}},
	abstract = {Although adversarial examples and model robustness have been extensively studied in the context of linear models and neural networks, research on this issue in tree-based models and how to make tree-based models robust against adversarial examples is still limited. In this paper, we show that tree based models are also vulnerable to adversarial examples and develop a novel algorithm to learn robust trees. At its core, our method aims to optimize the performance under the worstcase perturbation of input features, which leads to a max-min saddle point problem. Incorporating this saddle point objective into the decision tree building procedure is non-trivial due to the discrete nature of trees—a naive approach to ﬁnding the best split according to this saddle point objective will take exponential time. To make our approach practical and scalable, we propose efﬁcient tree building algorithms by approximating the inner minimizer in this saddle point problem, and present efﬁcient implementations for classical information gain based trees as well as state-of-the-art tree boosting models such as XGBoost. Experimental results on real world datasets demonstrate that the proposed algorithms can substantially improve the robustness of tree-based models against adversarial examples.},
	language = {en},
	author = {Chen, Hongge and Zhang, Huan and Boning, Duane and Hsieh, Cho-Jui},
	year = {2019},
	pages = {10},
	file = {Chen et al. - Robust Decision Trees Against Adversarial Examples.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/7WHJJ8DY/Chen et al. - Robust Decision Trees Against Adversarial Examples.pdf:application/pdf}
}

@inproceedings{russu_secure_2016,
	address = {Vienna, Austria},
	title = {Secure {Kernel} {Machines} against {Evasion} {Attacks}},
	isbn = {978-1-4503-4573-6},
	abstract = {Machine learning is widely used in security-sensitive settings like spam and malware detection, although it has been shown that malicious data can be carefully modiﬁed at test time to evade detection. To overcome this limitation, adversaryaware learning algorithms have been developed, exploiting robust optimization and game-theoretical models to incorporate knowledge of potential adversarial data manipulations into the learning algorithm. Despite these techniques have been shown to be eﬀective in some adversarial learning tasks, their adoption in practice is hindered by diﬀerent factors, including the diﬃculty of meeting speciﬁc theoretical requirements, the complexity of implementation, and scalability issues, in terms of computational time and space required during training. In this work, we aim to develop secure kernel machines against evasion attacks that are not computationally more demanding than their non-secure counterparts. In particular, leveraging recent work on robustness and regularization, we show that the security of a linear classiﬁer can be drastically improved by selecting a proper regularizer, depending on the kind of evasion attack, as well as unbalancing the cost of classiﬁcation errors. We then discuss the security of nonlinear kernel machines, and show that a proper choice of the kernel function is crucial. We also show that unbalancing the cost of classiﬁcation errors and varying some kernel parameters can further improve classiﬁer security, yielding decision functions that better enclose the legitimate data. Our results on spam and PDF malware detection corroborate our analysis.},
	language = {en},
	urldate = {2019-09-09},
	booktitle = {Proceedings of the 2016 {ACM} {Workshop} on {Artificial} {Intelligence} and {Security} - {ALSec} '16},
	publisher = {ACM Press},
	author = {Russu, Paolo and Demontis, Ambra and Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
	year = {2016},
	pages = {59--69},
	file = {Russu et al. - 2016 - Secure Kernel Machines against Evasion Attacks.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/G4XIDKGA/Russu et al. - 2016 - Secure Kernel Machines against Evasion Attacks.pdf:application/pdf}
}

@incollection{yosinski_how_2014,
	title = {How transferable are features in deep neural networks?},
	urldate = {2019-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
	year = {2014},
	pages = {3320--3328},
	file = {NIPS Full Text PDF:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/T56ZNBZ8/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf:application/pdf;NIPS Snapshot:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/CGKESR56/5347-how-transferable-are-features-in-deep-neural-networks.html:text/html}
}

@inproceedings{sharafaldin_toward_2018,
	address = {Funchal, Madeira, Portugal},
	title = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}:},
	shorttitle = {Toward {Generating} a {New} {Intrusion} {Detection} {Dataset} and {Intrusion} {Traffic} {Characterization}},
	abstract = {Intrusion Detection, IDS Dataset, DoS, Web Attack, Inﬁltration, Brute Force.},
	language = {en},
	urldate = {2019-09-10},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Information} {Systems} {Security} and {Privacy}},
	publisher = {SCITEPRESS},
	author = {Sharafaldin, Iman and Habibi Lashkari, Arash and Ghorbani, Ali A.},
	year = {2018},
	pages = {108--116},
	file = {Sharafaldin et al. - 2018 - Toward Generating a New Intrusion Detection Datase.pdf:/home/max/.zotero/zotero/zzt443zn.default/zotero/storage/H2PGUEGW/Sharafaldin et al. - 2018 - Toward Generating a New Intrusion Detection Datase.pdf:application/pdf}
}